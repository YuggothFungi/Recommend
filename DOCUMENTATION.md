# Документация проекта

## Содержание
1. [Обзор проекта](#обзор-проекта)
2. [Архитектура](#архитектура)
3. [Компоненты системы](#компоненты-системы)
4. [Обработка текста](#обработка-текста)
5. [База данных](#база-данных)
6. [API](#api)
7. [Развертывание](#развертывание)

## Структура проекта

### Основные компоненты

1. База данных
   - Основные таблицы
   - Таблицы для векторизации
   - Индексы и оптимизация

2. Обработка текста
   - Нормализация
   - Векторизация
   - Расчет сходства

3. API
   - Эндпоинты
   - Аутентификация
   - Документация

4. Интерфейс
   - Страницы
   - Компоненты
   - Стили

## Модули

### TextProcessor
Модуль для обработки и нормализации текста.

### MetricsAnalyzer
Модуль для анализа и сбора метрик качества системы.

#### Отчеты о метриках
Отчеты о метриках сохраняются в директории `reports/` в двух форматах:
- Текстовый формат (`metrics_YYYYMMDD_HHMMSS.txt`)
- JSON формат (`metrics_YYYYMMDD_HHMMSS.json`)

Содержимое отчетов:
- Метрики производительности
- Процент ошибок
- История метрик
- Временная метка генерации

## Обзор проекта

Проект представляет собой систему рекомендаций, основанную на анализе текстовых данных. Система использует различные методы обработки естественного языка для извлечения ключевой информации из текстов и формирования рекомендаций.

## Архитектура

Проект построен на основе модульной архитектуры с четким разделением ответственности между компонентами:

```
project/
├── src/                    # Исходный код
│   ├── text_processor.py   # Обработка текста
│   ├── database.py        # Работа с БД
│   ├── api.py             # API endpoints
│   └── utils.py           # Вспомогательные функции
├── database/              # Скрипты БД
├── tests/                 # Тесты
├── frontend/             # Веб-интерфейс
└── input/                # Входные данные
```

## Компоненты системы

### Обработка текста (text_processor.py)

Модуль отвечает за нормализацию и обработку текстовых данных. Основные функции:

- Нормализация текста
- Лемматизация
- Обработка составных терминов
- Удаление стоп-слов

#### Нормализация текста

Процесс нормализации включает следующие шаги:
1. Приведение к нижнему регистру
2. Замена составных терминов
3. Удаление специальных символов
4. Токенизация
5. Удаление стоп-слов
6. Лемматизация

#### Лемматизация

Используется библиотека pymorphy2 для приведения слов к нормальной форме. Особенности:
- Приоритет существительных при лемматизации
- Обработка исключений для специальных терминов
- Сохранение составных терминов

#### Составные термины

Система поддерживает обработку составных терминов через словарь `DOMAIN_PHRASES`. Примеры:
- "программное обеспечение"
- "база данных"
- "система управления"

### База данных (database.py)

Модуль обеспечивает работу с SQLite базой данных. Основные функции:
- Инициализация БД
- Загрузка данных
- Обновление записей
- Поиск и фильтрация

### API (api.py)

REST API для взаимодействия с системой. Основные эндпоинты:
- `/api/texts` - работа с текстами
- `/api/recommendations` - получение рекомендаций
- `/api/statistics` - статистика

## Обработка текста

### Нормализация

1. Приведение к нижнему регистру
2. Удаление пунктуации
3. Лемматизация
4. Удаление стоп-слов
5. Обработка составных терминов

### Векторизация

#### Классы

1. `VectorizationConfig`
   - Управление конфигурациями векторизации
   - Загрузка конфигураций из БД
   - Получение весов для сущностей

2. `VectorizationTextWeights`
   - Обработка весов текста
   - Подготовка текста с учетом весов
   - Учет часов в весах

3. `DatabaseVectorizer`
   - Обертка для работы с векторизаторами
   - Поддержка TF-IDF и ruBERT
   - Сохранение результатов в БД

#### Методы векторизации

1. TF-IDF
   - Векторизация на основе частоты терминов
   - Учет важности терминов в корпусе
   - Нормализация векторов

2. ruBERT
   - Векторизация на основе языковой модели
   - Учет контекста и семантики
   - Нормализация векторов

### Расчет сходства

1. Косинусное сходство
   - Нормализация векторов
   - Расчет скалярного произведения
   - Учет угла между векторами

2. Учет весов
   - Веса для текстовых компонентов
   - Веса для часов
   - Комбинирование весов

## База данных

### Основные таблицы

1. `disciplines` - дисциплины
2. `semesters` - семестры
3. `sections` - разделы
4. `lecture_topics` - темы лекций
5. `practical_topics` - темы практик
6. `self_control_questions` - вопросы для самоконтроля
7. `competencies` - компетенции
8. `specialties` - специальности
9. `labor_functions` - трудовые функции

### Таблицы для векторизации

1. `vectorization_configurations`
   - `id` - идентификатор конфигурации
   - `name` - название конфигурации
   - `description` - описание
   - `config_type` - тип конфигурации (lecture, practical, labor_function)
   - `created_at` - дата создания

2. `vectorization_weights`
   - `id` - идентификатор веса
   - `configuration_id` - ссылка на конфигурацию
   - `entity_type` - тип сущности (lecture_topic, practical_topic, labor_function)
   - `source_type` - тип источника текста (name, description, context)
   - `use_normalized` - использовать нормализованный текст
   - `weight` - вес для текста
   - `hours_weight` - вес для часов

3. `vectorization_results`
   - `id` - идентификатор результата
   - `configuration_id` - ссылка на конфигурацию
   - `entity_type` - тип сущности
   - `entity_id` - идентификатор сущности
   - `vector_type` - тип вектора (tfidf, rubert)
   - `vector_data` - данные вектора
   - `created_at` - дата создания
   
   Примечание: Для каждой сущности могут существовать несколько векторов разных типов (TF-IDF и ruBERT) 
   в рамках одной конфигурации. Это позволяет сравнивать результаты разных методов векторизации.

4. `similarity_results`
   - `id` - идентификатор результата
   - `configuration_id` - ссылка на конфигурацию
   - `topic_id` - идентификатор темы
   - `topic_type` - тип темы (lecture, practical)
   - `labor_function_id` - идентификатор трудовой функции
   - `rubert_similarity` - сходство по ruBERT
   - `tfidf_similarity` - сходство по TF-IDF
   - `topic_hours` - часы темы
   - `created_at` - дата создания
   
   Примечание: При расчете сходства для каждого метода векторизации (TF-IDF или ruBERT) 
   сохраняются оба значения сходства, но для неиспользуемого метода устанавливается 0.0. 
   Это позволяет в дальнейшем комбинировать результаты разных методов.

### Процесс векторизации

1. Подготовка текстов
   - Нормализация текстов
   - Применение весов к компонентам текста
   - Учет часов в весах

2. Векторизация
   - TF-IDF векторизация
   - ruBERT векторизация
   - Сохранение обоих типов векторов

3. Расчет сходства
   - Группировка векторов по типам
   - Расчет косинусного сходства для каждого типа
   - Сохранение результатов с указанием типа векторизации

### Комбинирование результатов

1. Возможности
   - Сравнение результатов разных методов
   - Взвешенное комбинирование сходства
   - Анализ эффективности методов

2. Ограничения
   - Необходимость наличия векторов обоих типов
   - Зависимость от качества нормализации текста
   - Влияние выбранных весов на результат

## API

### Эндпоинты

1. Конфигурации
   - GET /api/configurations - список конфигураций
   - GET /api/configurations/{id} - детали конфигурации
   - POST /api/configurations - создание конфигурации
   - PUT /api/configurations/{id} - обновление конфигурации
   - DELETE /api/configurations/{id} - удаление конфигурации

2. Векторизация
   - POST /api/vectorize - векторизация текстов
   - GET /api/vectorize/{id} - статус векторизации
   - GET /api/vectorize/{id}/results - результаты векторизации

3. Сходство
   - POST /api/similarity - расчет сходства
   - GET /api/similarity/{id} - статус расчета
   - GET /api/similarity/{id}/results - результаты расчета

## Развертывание

### Требования

1. Python 3.8+
2. PostgreSQL 12+
3. CUDA (для ruBERT)

### Установка

1. Клонирование репозитория
2. Установка зависимостей
3. Настройка БД
4. Запуск приложения

### Конфигурация

1. Настройка БД
2. Настройка API
3. Настройка интерфейса
4. Настройка безопасности

# Документация по классам и методам

## Векторизация текста

### TfidfDatabaseVectorizer
Класс TfidfDatabaseVectorizer описывает объект "Векторизатор TF-IDF", реализует действия: "векторизация" для "преобразования текстов в векторы с использованием TF-IDF", "сохранение" для "сохранения векторов в базе данных".

#### Методы:
- `__init__(config: VectorizationConfig = None)` - инициализация векторизатора с конфигурацией
- `fit(texts: List[str])` - обучение векторизатора на текстах
- `transform(texts: List[str])` - преобразование текстов в векторы
- `fit_transform(texts: List[str])` - обучение и преобразование текстов в векторы
- `vectorize_all(conn=None)` - векторизация всех текстов в базе данных
- `_save_meta()` - сохранение метаданных векторизатора
- `_get_topic_texts(cursor)` - получение объединенных нормализованных текстов тем
- `_get_labor_function_texts(cursor)` - получение объединенных нормализованных текстов трудовых функций
- `_save_vector(cursor, entity_type: str, entity_id: int, vector)` - сохранение вектора в базу данных
