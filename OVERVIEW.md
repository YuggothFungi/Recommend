# Обзор проекта

## Описание

Проект представляет собой систему рекомендаций, основанную на анализе текстовых данных. Система использует различные методы обработки естественного языка для извлечения ключевой информации из текстов и формирования рекомендаций.

## Основные возможности

### Обработка текста
- Нормализация текста
- Лемматизация с использованием pymorphy2
- Обработка составных терминов
- Удаление стоп-слов
- Сохранение специальных терминов

### База данных
- Хранение исходных и нормализованных текстов
- Управление рекомендациями
- Сбор статистики
- Оптимизированные запросы

### API
- REST API для взаимодействия
- Валидация данных
- Обработка ошибок
- Документированные эндпоинты

## Технический стек

### Бэкенд
- Python 3.8+
- FastAPI
- SQLite
- pymorphy2
- NLTK

### Фронтенд
- React
- TypeScript
- Material-UI
- Chart.js

### Инфраструктура
- Git
- GitHub Actions
- Docker
- SQLite

## Архитектура

### Модули
1. **Обработка текста**
   - Нормализация
   - Лемматизация
   - Обработка составных терминов

2. **База данных**
   - Хранение данных
   - Миграции
   - Оптимизация

3. **API**
   - Эндпоинты
   - Валидация
   - Обработка ошибок

4. **Фронтенд**
   - Интерфейс
   - Визуализация
   - Взаимодействие

### Процессы
1. **Обработка текста**
   - Получение текста
   - Нормализация
   - Лемматизация
   - Сохранение

2. **Формирование рекомендаций**
   - Анализ текста
   - Поиск соответствий
   - Генерация рекомендаций

3. **Взаимодействие с пользователем**
   - Получение запроса
   - Обработка
   - Отправка результата

## Текущее состояние

### Реализовано
- Базовая нормализация текста
- Лемматизация с pymorphy2
- Обработка составных терминов
- Базовые операции с БД
- REST API

### В разработке
- Улучшение точности
- Тестирование
- Документация
- Расширение функциональности

### Планируется
- Улучшение пользовательского опыта
- Масштабирование
- Интеграция с внешними системами

## Метрики

### Производительность
- Время обработки текста < 100мс
- Время ответа API < 200мс
- Использование памяти < 500MB

### Точность
- Точность нормализации > 95%
- Точность лемматизации > 90%
- Точность рекомендаций > 85%

### Надежность
- Доступность системы > 99.9%
- Время восстановления < 5 минут
- Количество ошибок < 0.1%

## Дорожная карта

### Краткосрочные цели
1. Улучшение точности
2. Тестирование
3. Документация
4. Расширение функциональности

### Среднесрочные цели
1. Улучшение пользовательского опыта
2. Масштабирование
3. Интеграция с внешними системами

### Долгосрочные цели
1. Оптимизация производительности
2. Расширение аналитических возможностей
3. Улучшение пользовательского опыта
4. Интеграция с внешними системами

## Команда

### Роли
- Разработчики
- Тестировщики
- Документаторы
- Менеджеры

### Ответственности
- Разработка
- Тестирование
- Документирование
- Управление

## Ресурсы

### Документация
- API документация
- Руководства
- Примеры
- Архитектура

### Код
- Репозиторий
- Тесты
- Примеры
- Документация

### Инфраструктура
- CI/CD
- Мониторинг
- Логирование
- Бэкапы

## Архитектура системы

### Основные компоненты
1. **Обработка данных** (`data_processor.py`)
   - Центральный модуль обработки данных
   - Последовательное выполнение всех этапов
   - Поддержка TF-IDF и ruBERT векторизации
   - Расчет метрик сходства

2. **Загрузка данных** (`data_loader.py`)
   - Загрузка компетенций
   - Загрузка трудовых функций
   - Загрузка учебных планов
   - Инициализация связей между данными

3. **Обработка текста** (`text_processor.py`)
   - Нормализация текстов
   - Удаление стоп-слов
   - Подготовка текстов к векторизации

4. **Векторизация** (`vectorizer.py`, `tfidf_vectorizer.py`, `rubert_vectorizer.py`)
   - TF-IDF векторизация
   - ruBERT векторизация
   - Расчет сходства между векторами

### Процесс обработки данных
1. Инициализация базы данных
   - Создание таблиц
   - Предзаполнение справочников (семестры, типы компонентов)
   - Создание индексов

2. Загрузка исходных данных
   - Компетенции из competencies.json
   - Трудовые функции из prof_std.json
   - Учебные планы из curriculum_disciplines
   - Создание связей между данными

3. Обработка и нормализация текстов
   - Приведение к нижнему регистру
   - Удаление спецсимволов
   - Удаление стоп-слов
   - Лемматизация

4. Векторизация текстов
   - TF-IDF векторизация
   - ruBERT векторизация
   - Сохранение векторов в БД

5. Расчет сходства
   - Косинусное сходство для TF-IDF
   - Косинусное сходство для ruBERT
   - Сохранение результатов

6. Генерация рекомендаций
   - Анализ сходства
   - Формирование рекомендаций
   - Сохранение в БД

### Проверка данных
- Валидация загруженных данных
- Проверка векторов
- Проверка расчетов сходства
- Верификация нормализованных текстов

## Технологический стек
- Python 3.8+
- SQLite
- NLTK для обработки текста
- scikit-learn для TF-IDF
- transformers для ruBERT
- Flask для API
- HTML/CSS/JavaScript для фронтенда

## Основные функции
1. Загрузка и обработка учебных планов
2. Анализ компетенций и трудовых функций
3. Векторизация и расчет сходства
4. Генерация рекомендаций
5. Визуализация результатов

## Метрики качества
1. Точность векторизации
2. Качество рекомендаций
3. Время обработки данных
4. Устойчивость к ошибкам

# Рекомендательная система сопоставления учебных программ и профессиональных стандартов

## Цель проекта
Создание прототипа системы для автоматического выявления взаимосвязей между темами учебных дисциплин и требованиями профессиональных стандартов на основе семантического анализа текстовых данных.
Выдача рекомендаций пользователю о возможных изменения в программе учебной дисциплины для лучшего соответствия профессиональному стандарту.

---

## Обновлённый список задач

### 1. Анализ и структурирование данных
1.1. Извлечение данных из JSON-файлов:
- Компетенции (`abilities.json`)
- Трудовые функции (`prof_std.json`)
- Темы рабочей программы (`database.json`)

1.2. Проектирование и реализация структуры БД:
- Создание нормализованной схемы данных
- Реализация связей "многие-ко-многим"
- Парсинг и загрузка данных в SQLite

1.3. Дополнительные операции:
- Создание индексов для оптимизации запросов
- Экспорт дампа БД для резервного копирования

---

### 2. Предобработка текста
2.1. Нормализация:
- Приведение к нижнему регистру
- Удаление спецсимволов и стоп-слов
- Обработка многозначных терминов (контекстный словарь)

2.2. Лемматизация:
- Использование `pymorphy2` для русского языка
- Токенизация с помощью `nltk`

---

### 3. Векторизация текста
3.1. Выбор метода:
- **TF-IDF** (быстрый старт, интерпретируемость)
- Интеграция веса часов обучения
- Кастомный токенизатор с учётом морфологии

---

### 4. Определение метрики близости
4.1. Реализация:
- Косинусное сходство между:
  - Векторами тем (учебная программа)
  - Агрегированными векторами трудовых функций
- Учёт пересечений компонентов (действия/умения/знания)

---

### 5. Рекомендательная логика
5.1. Алгоритм:
- Для каждой темы → Топ-3 трудовых функций
- Для дисциплины → Анализ распределения сходства

5.2. Визуализация:
- Интерактивные таблицы связи "Тема-Функция"
- Heatmap распределения компетенций

---

### 6. Реализация интерфейса
6.1. Backend (Flask):
- Добавить API-роуты для получения списка дисциплин, тем, трудовых функций и их сходства.
- Реализовать логику рекомендаций на сервере.

6.2. Frontend:
- Реализовать split-view интерфейс с выбором дисциплины, фильтрацией тем, слайдером порога сходства и панелью рекомендаций.
- JS: при выборе строки отправлять запрос на сервер, получать релевантные элементы и подсвечивать их.

---

## Технологический стек
| Этап | Технологии | Пакеты |
|-------|------------|--------|
| **Хранение данных** | SQLite | `sqlite3` |
| **Обработка текста** | Python | `pymorphy2`, `nltk` |
| **Векторизация** | Scikit-learn | `TfidfVectorizer` |
| **Backend** | Flask | `flask` |
| **Frontend** | HTML, CSS, JS | - |

---

## Структура базы данных
### Основные таблицы
| Таблица | Описание |
|---------|----------|
| `topics` | Учебные темы (название, часы, семестр) |
| `competencies` | Компетенции (код, категория, описание) |
| `labor_functions` | Трудовые функции (ID, уровень квалификации) |
| `labor_components` | Универсальные компоненты (действия/умения/знания) |
| `component_types` | Типы компонентов (справочник) |

### Таблицы связей
| Таблица | Назначение |
|---------|------------|
| `topic_competency` | Связь тем и компетенций |
| `labor_function_components` | Связь функций и компонентов |
| `topic_labor_function` | Результаты сопоставления (сходство) |

### Векторные представления
| Таблица | Назначение |
|---------|------------|
| `tfidf_vectors` | TF-IDF векторы для всех текстов |
| `rubert_vectors` | ruBERT векторы для семантического анализа |
| `normalized_texts` | Нормализованные тексты для поиска |

## Структура данных

### Основные сущности
1. **Дисциплины**
   - Основная информация (название, цели, задачи)
   - Связь с семестрами
   - Связь с компетенциями

2. **Разделы**
   - Привязаны к дисциплинам и семестрам
   - Содержат темы лекций и практических занятий
   - Включают вопросы для самоконтроля

3. **Темы**
   - Лекционные темы
   - Темы практических занятий
   - Количество часов
   - Векторные представления

4. **Компетенции**
   - Код и категория
   - Описание
   - Связь с дисциплинами

5. **Трудовые функции**
   - Код и название
   - Уровень квалификации
   - Компоненты (действия/умения/знания)

### Процесс обработки данных
1. **Инициализация базы данных**
   - Создание таблиц
   - Предзаполнение справочников (семестры, типы компонентов)

2. **Загрузка данных**
   - Загрузка дисциплин и их разделов
   - Загрузка тем лекций и практических занятий
   - Загрузка компетенций
   - Загрузка трудовых функций

3. **Обработка текстов**
   - Нормализация названий и описаний
   - Векторизация текстов (TF-IDF, ruBERT)
   - Расчет сходства между сущностями

4. **Анализ и рекомендации**
   - Сопоставление тем и трудовых функций
   - Анализ компетенций по семестрам
   - Формирование рекомендаций

### Векторные представления
1. **TF-IDF векторы**
   - Для текстовых полей
   - Нормализованные представления
   - Расчет сходства

2. **ruBERT векторы**
   - Семантические представления
   - Учет контекста
   - Более точное сходство